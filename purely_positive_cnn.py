# -*- coding: utf-8 -*-
"""purely_positive_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Ko5xE41FFtzEl2SS01Xri5jyCHSAytT
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.distributions as dist
from copy import deepcopy
import math
import matplotlib.pyplot as plt
import torch.optim as opt

global DEVICE
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

### General Utils ###
def boolcheck(x):
    return str(x).lower() in ["true", "1", "yes"]

def set_tensor(xs):
    return xs.float().to(DEVICE)

def edge_zero_pad(img,d):
  N,C, h,w = img.shape
  x = torch.zeros((N,C,h+(d*2),w+(d*2))).to(DEVICE)
  x[:,:,d:h+d,d:w+d] = img
  return x

def accuracy(out, L):
  B,l = out.shape
  total = 0
  for i in range(B):
    if torch.argmax(out[i,:]) == torch.argmax(L[i,:]):
      total +=1
  return total/ B

def sequence_accuracy(model, target_batch):
    accuracy = 0
    L = len(target_batch)
    _,B = target_batch[0].shape
    s = ""
    for i in range(len(target_batch)): # this loop is over the seq_len
      s += str(torch.argmax(model.mu_y[i][:,0]).item()) + " " + str(torch.argmax(target_batch[i][:,0]).item()) + "  "
      for b in range(B):
        #print("target idx: ", torch.argmax(target_batch[i][:,b]).item())
        #print("pred idx: ", torch.argmax(model.mu_y[i][:,b]).item())
        if torch.argmax(target_batch[i][:,b]) ==torch.argmax(model.mu_y[i][:,b]):
          accuracy+=1
    print("accs: ", s)
    return accuracy / (L * B)

def custom_onehot(idx, shape):
  ret = set_tensor(torch.zeros(shape))
  ret[idx] =1
  return ret

def onehot(arr, vocab_size):
  L, B = arr.shape
  ret = np.zeros([L,vocab_size,B])
  for l in range(L):
    for b in range(B):
      ret[l,int(arr[l,b]),b] = 1
  return ret

def inverse_list_onehot(arr):
  L = len(arr)
  V,B = arr[0].shape
  ret = np.zeros([L,B])
  for l in range(L):
    for b in range(B):
      for v in range(V):
        if arr[l][v,b] == 1:
          ret[l,b] = v
  return ret

def decode_ypreds(ypreds):
  L = len(ypreds)
  V,B = ypreds[0].shape
  ret = np.zeros([L,B])
  for l in range(L):
    for b in range(B):
      v = torch.argmax(ypreds[l][:,b])
      ret[l,b] =v
  return ret


def inverse_onehot(arr):
  if type(arr) == list:
    return inverse_list_onehot(arr)
  else:
    L,V,B = arr.shape
    ret = np.zeros([L,B])
    for l in range(L):
      for b in range(B):
        for v in range(V):
          if arr[l,v,b] == 1:
            ret[l,b] = v
    return ret

### Activation functions ###
def tanh(xs):
    return torch.tanh(xs)

def linear(x):
    return x

def tanh_deriv(xs):
    return 1.0 - torch.tanh(xs) ** 2.0

def linear_deriv(x):
    return set_tensor(torch.ones((1,)))

def relu(xs):
  return torch.clamp(xs,min=0)

def relu_deriv(xs):
  rel = relu(xs)
  rel[rel>0] = 1
  return rel

def softmax(xs):
  return F.softmax(xs)

def sigmoid(xs):
  return F.sigmoid(xs)

def sigmoid_deriv(xs):
  return F.sigmoid(xs) * (torch.ones_like(xs) - F.sigmoid(xs))


### loss functions
def mse_loss(out, label):
      return torch.sum((out-label)**2)

def mse_deriv(out,label):
      return 2 * (out - label)

ce_loss = nn.CrossEntropyLoss()

def cross_entropy_loss(out,label):
      return ce_loss(out,label)

def my_cross_entropy(out,label):
      return -torch.sum(label * torch.log(out + 1e-6))

def cross_entropy_deriv(out,label):
      return out - label

def parse_loss_function(loss_arg):
      if loss_arg == "mse":
            return mse_loss, mse_deriv
      elif loss_arg == "crossentropy":
            return my_cross_entropy, cross_entropy_deriv
      else:
            raise ValueError("loss argument not expected. Can be one of 'mse' and 'crossentropy'. You inputted " + str(loss_arg))


### Initialization Functions ###
def gaussian_init(W,mean=0.0, std=0.05):
  return W.normal_(mean=0.0,std=0.05)

def zeros_init(W):
  return torch.zeros_like(W)

def kaiming_init(W, a=math.sqrt(5),*kwargs):
  return init.kaiming_uniform_(W, a)

def glorot_init(W):
  return init.xavier_normal_(W)

def kaiming_bias_init(b,*kwargs):
  fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
  bound = 1 / math.sqrt(fan_in)
  return init.uniform_(b, -bound, bound)

#the initialization pytorch uses for lstm
def std_uniform_init(W,hidden_size):
  stdv = 1.0 / math.sqrt(hidden_size)
  return init.uniform_(W, -stdv, stdv)

import tensorflow as tf
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torchvision
import torchvision.transforms as transforms
import subprocess

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

def show_dataset(dataset):
    images, labels = dataset[0]
    print("IMAGES: ", images.shape)
    print("LABELS: ", labels.shape)
    print(onehot(labels))
    # show images
    imshow(torchvision.utils.make_grid(images))
    # print labels
    print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

def get_cnn_dataset(dataset, batch_size):
    transform = transforms.Compose([transforms.ToTensor()])#, transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])
    if dataset == "cifar":
        trainset = torchvision.datasets.CIFAR10(root='./cifar_data', train=True,
                                                download=True, transform=transform)
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True)
        train_data = list(iter(trainloader))
        testset = torchvision.datasets.CIFAR10(root='./cifar_data', train=False,
                                               download=True, transform=transform)
        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                          shuffle=True)
        test_data = list(iter(testloader))
    elif dataset == "cifar100":
        trainset = torchvision.datasets.CIFAR100(root='./cifar100_data', train=True,
                                                download=False, transform=transform)
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True)
        train_data = list(iter(trainloader))
        testset = torchvision.datasets.CIFAR100(root='./cifar100_data', train=False,
                                               download=False, transform=transform)
        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                          shuffle=True)
        test_data = list(iter(testloader))
    elif dataset == "svhn":
        trainset = torchvision.datasets.SVHN(root='./svhn_data', split='train',
                                                download=False, transform=transform)
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True)
        train_data = list(iter(trainloader))
        testset = torchvision.datasets.SVHN(root='./svhn_data', split='test',
                                               download=False, transform=transform)
        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                          shuffle=True)
        test_data = list(iter(testloader))
    elif dataset == "mnist":
        mnist_transform = transforms.Compose([transforms.ToTensor(), mnist_normalize])
        trainset = torchvision.datasets.MNIST(root='./mnist_data', train=True,
                                                download=False, transform=mnist_transform)
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True)
        train_data = list(iter(trainloader))
        testset = torchvision.datasets.MNIST(root='./mnist_data', train=False,
                                               download=False, transform=mnist_transform)
        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                          shuffle=True)
        test_data = list(iter(testloader))
    else:
        raise Exception("dataset: " + str(dataset) + " not supported")

    print("Setup data:")
    print("Train: ",len(train_data))
    print("Test: ", len(test_data))
    return train_data, test_data

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

def get_lstm_dataset(seq_length, batch_size,buffer_size=10000):
    path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
    text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
    vocab = sorted(set(text))
    char2idx = {u:i for i, u in enumerate(vocab)}
    idx2char = np.array(vocab)
    text_as_int = np.array([char2idx[c] for c in text])
    examples_per_epoch = len(text)//(seq_length+1)

    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)
    dataset = sequences.map(split_input_target)

    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)
    dataset = list(iter(dataset))
    #get dataset in right format
    vocab_size = len(vocab)
    return dataset, vocab_size,char2idx,idx2char

class ConvLayer(nn.Module):
  def __init__(self,input_size,num_channels,num_filters,batch_size,kernel_size,learning_rate,f,df,purely_positive=False, use_layer_norm=True,padding=0,stride=1,device="cpu"):
    super(ConvLayer, self).__init__()
    self.input_size = input_size
    self.num_channels = num_channels
    self.num_filters = num_filters
    self.batch_size = batch_size
    self.kernel_size = kernel_size
    self.padding = padding
    self.stride = stride
    self.output_size = math.floor((self.input_size + (2 * self.padding) - self.kernel_size)/self.stride) +1
    self.learning_rate = learning_rate
    self.f = f
    self.df = df
    self.device = device
    self.kernel= nn.Parameter(torch.empty(self.num_filters,self.num_channels,self.kernel_size,self.kernel_size).normal_(mean=0,std=0.05))
    self.unfold = nn.Unfold(kernel_size=(self.kernel_size,self.kernel_size),padding=self.padding,stride=self.stride)
    self.fold = nn.Fold(output_size=(self.input_size,self.input_size),kernel_size=(self.kernel_size,self.kernel_size),padding=self.padding,stride=self.stride)
    self.purely_positive = purely_positive
    self.use_layer_norm = use_layer_norm
    self.layer_norm = nn.LayerNorm((self.batch_size, self.num_filters, self.output_size, self.output_size))
    self.bias = nn.Parameter(torch.zeros((self.batch_size, self.num_filters, self.output_size, self.output_size)))
    #self.kernel = self.kernel.to(self.device)
    #self.unfold = self.unfold.to(self.device)
    #self.fold = self.fold.to(self.device)
    #self.bias = self.bias.to(self.device)

  def forward(self,inp):
    #print("passing through conv forward")
    if self.purely_positive and (inp < 0).any():
      print(" negative inputs!!!")
    self.X_col = self.unfold(inp.clone())
    if self.purely_positive and (self.X_col < 0).any():
      print(" negative XCOL")
    self.flat_weights = self.kernel.reshape(self.num_filters,-1)
    if self.purely_positive:
      self.flat_weights = torch.exp(self.flat_weights)
      #print("PURELY POSITIVE")
      if self.purely_positive and (self.flat_weights < 0).any():
        print("NEGATIVE FLAT WEIGHTS")
    out = self.flat_weights @ self.X_col
    self.activations = out.reshape(self.batch_size, self.num_filters, self.output_size, self.output_size)
    if self.purely_positive and (self.activations < 0).any():
        print("Negative activations found")
        #print(self.activations)
        #print(x)
        #print(torch.exp(self.weights))
        crash
    if self.use_layer_norm:
      return self.f(self.layer_norm(self.activations + torch.exp(self.bias)))
    return self.f(self.activations + torch.exp(self.bias))

  def update_weights(self,e,update_weights=False,sign_reverse=False):
    fn_deriv = self.df(self.activations)
    e = e * fn_deriv
    self.dout = e.reshape(self.batch_size,self.num_filters,-1)
    dW = self.dout @ self.X_col.permute(0,2,1)
    dW = torch.sum(dW,dim=0)
    dW = dW.reshape((self.num_filters,self.num_channels,self.kernel_size,self.kernel_size))
    if update_weights:
      if sign_reverse==True: # This is necessary because PC and backprop learn gradients with different signs grad_pc = -grad_bp
        self.kernel -= self.learning_rate * torch.clamp(dW * 2,-50,50)
      else:
        self.kernel += self.learning_rate * torch.clamp(dW * 2,-50,50)
    return dW

  def backward(self,e):
    fn_deriv = self.df(self.activations)
    e = e * fn_deriv
    self.dout = e.reshape(self.batch_size,self.num_filters,-1)
    dX_col = self.flat_weights.T @ self.dout
    dX = self.fold(dX_col)
    return torch.clamp(dX,-50,50)

  def get_true_weight_grad(self):
    return self.kernel.grad

  def set_weight_parameters(self):
    self.kernel = nn.Parameter(self.kernel)

  def save_layer(self,logdir,i):
      np.save(logdir +"/layer_"+str(i)+"_weights.npy",self.kernel.detach().cpu().numpy())

  def load_layer(self,logdir,i):
    kernel = np.load(logdir +"/layer_"+str(i)+"_weights.npy")
    self.kernel = set_tensor(torch.from_numpy(kernel))

class MaxPool(nn.Module):
  def __init__(self, kernel_size,device='cpu'):
    super(MaxPool, self).__init__()
    self.kernel_size = kernel_size
    self.device = device
    self.activations = torch.empty(1)

  def forward(self,x):
    out, self.idxs = F.max_pool2d(x, self.kernel_size,return_indices=True)
    return out

  def backward(self, y):
    return F.max_unpool2d(y,self.idxs, self.kernel_size)

  def update_weights(self,e,update_weights=False,sign_reverse=False):
    return 0

  def get_true_weight_grad(self):
    return None

  def set_weight_parameters(self):
    pass

  def save_layer(self,logdir,i):
    pass

  def load_layer(self,logdir,i):
    pass

class AvgPool(nn.Module):
  def __init__(self, kernel_size,device='cpu'):
    super(AvgPool, self).__init__()
    self.kernel_size = kernel_size
    self.device = device
    self.activations = torch.empty(1)

  def forward(self, x):
    self.B_in,self.C_in,self.H_in,self.W_in = x.shape
    return F.avg_pool2d(x,self.kernel_size)

  def backward(self, y):
    N,C,H,W = y.shape
    print("in backward: ", y.shape)
    return F.interpolate(y,scale_factor=(1,1,self.kernel_size,self.kernel_size))

  def update_weights(self,e,update_weights=False, sign_reverse=False):
    return 0

  def save_layer(self,logdir,i):
    pass

  def load_layer(self,logdir,i):
    pass

class ProjectionLayer(nn.Module):
  def __init__(self,input_size, output_size,f,df,learning_rate,purely_positive=False, use_layer_norm=True,device='cpu'):
    super(ProjectionLayer, self).__init__()
    self.input_size = input_size
    self.B, self.C, self.H, self.W = self.input_size
    self.output_size =output_size
    self.learning_rate = learning_rate
    self.f = f
    self.df = df
    self.device = device
    self.Hid = self.C * self.H * self.W
    self.weights = nn.Parameter(torch.empty((self.Hid, self.output_size)).normal_(mean=0.0, std=0.05).to(self.device))
    self.bias = nn.Parameter(torch.zeros((self.B, self.output_size)).to(self.device))
    self.purely_positive = purely_positive
    self.use_layer_norm = use_layer_norm
    self.layer_norm = torch.nn.LayerNorm(output_size)

  def forward(self, x):
    self.inp = x.detach().clone()
    out = x.reshape((len(x), -1))
    if self.purely_positive:
      self.activations = torch.matmul(out,torch.exp(self.weights))
      if self.purely_positive and (self.activations < 0).any():
        print("Negative found!")
        print(self.activations)
        print(x)
        print(torch.exp(self.weights))
        crash
    else:
      self.activations = torch.matmul(out, self.weights)
    if self.use_layer_norm:
      return self.f(self.layer_norm(self.activations + torch.exp(self.bias)))
    else:
      return self.f(self.activations + torch.exp(self.bias))

  def backward(self, e):
    fn_deriv = self.df(self.activations)
    out = torch.matmul(e * fn_deriv, self.weights.T)
    out = out.reshape((len(e), self.C, self.H, self.W))
    return torch.clamp(out,-50,50)

  def update_weights(self, e,update_weights=False,sign_reverse=False):
    out = self.inp.reshape((len(self.inp), -1))
    fn_deriv = self.df(self.activations)
    dw = torch.matmul(out.T, e * fn_deriv)
    if update_weights:
      if sign_reverse==True:
        self.weights -= self.learning_rate * torch.clamp((dw * 2),-50,50)
      else:
        self.weights += self.learning_rate * torch.clamp((dw * 2),-50,50)
    return dw

  def get_true_weight_grad(self):
    return self.weights.grad

  def set_weight_parameters(self):
    self.weights = nn.Parameter(self.weights)

  def save_layer(self,logdir,i):
    np.save(logdir +"/layer_"+str(i)+"_weights.npy",self.weights.detach().cpu().numpy())

  def load_layer(self,logdir,i):
    weights = np.load(logdir +"/layer_"+str(i)+"_weights.npy")
    self.weights = set_tensor(torch.from_numpy(weights))

class FCLayer(nn.Module):
  def __init__(self, input_size,output_size,batch_size, learning_rate,f,df,purely_positive=False, use_layer_norm=True,device="cpu"):
    super(FCLayer, self).__init__()
    self.input_size = input_size
    self.output_size = output_size
    self.batch_size = batch_size
    self.learning_rate = learning_rate
    self.f = f
    self.df = df
    self.device = device
    self.purely_positive = purely_positive
    self.use_layer_norm = use_layer_norm
    self.layer_norm = torch.nn.LayerNorm(output_size)
    self.weights = nn.Parameter(torch.empty([self.input_size,self.output_size]).normal_(mean=0.0,std=0.05).to(self.device))
    self.bias = nn.Parameter(torch.zeros(self.batch_size, self.output_size).to(self.device))

  def forward(self,x):
    self.inp = x.clone()
    if self.purely_positive:
        self.activations = torch.matmul(self.inp, torch.exp(self.weights))
        if self.purely_positive and (self.activations < 0).any():
          print("Negative found!")
          print(self.activations)
          print(x)
          print(torch.exp(self.weights))
          crash
    else:
      self.activations = torch.matmul(self.inp, self.weights)
    if self.use_layer_norm:
      return self.f(self.layer_norm(self.activations + torch.exp(self.bias)))
    else:
      return self.f(self.activations + torch.exp(self.bias))

  def backward(self,e):
    self.fn_deriv = self.df(self.activations)
    out = torch.matmul(e * self.fn_deriv, self.weights.T)
    return torch.clamp(out,-50,50)

  def update_weights(self,e,update_weights=False,sign_reverse=False):
    self.fn_deriv = self.df(self.activations)
    dw = torch.matmul(self.inp.T, e * self.fn_deriv)
    if update_weights:
      if sign_reverse==True:
        self.weights -= self.learning_rate * torch.clamp(dw*2,-50,50)
      else:
        self.weights += self.learning_rate * torch.clamp(dw*2,-50,50)
    return dw

  def get_true_weight_grad(self):
    return self.weights.grad

  def set_weight_parameters(self):
    self.weights = nn.Parameter(self.weights)

  def save_layer(self,logdir,i):
    np.save(logdir +"/layer_"+str(i)+"_weights.npy",self.weights.detach().cpu().numpy())

  def load_layer(self,logdir,i):
    weights = np.load(logdir +"/layer_"+str(i)+"_weights.npy")
    self.weights = set_tensor(torch.from_numpy(weights))

import datetime

class Backprop_CNN(nn.Module):
  def __init__(self, layers,loss_fn,loss_fn_deriv, purely_positive = False,use_layer_norm=True):
    super(Backprop_CNN, self).__init__()
    self.layers = layers
    self.xs = [[] for i in range(len(self.layers)+1)]
    self.e_ys = [[] for i in range(len(self.layers)+1)]
    self.loss_fn = loss_fn
    self.loss_fn_deriv = loss_fn_deriv
    self.purely_positive = purely_positive
    self.use_layer_norm = use_layer_norm
    for l in self.layers:
      l.set_weight_parameters()
      l.purely_positive = self.purely_positive
      l.use_layer_norm = self.use_layer_norm

  def forward(self, inp):
    self.xs[0] = inp
    for i,l in enumerate(self.layers):
      self.xs[i+1] = l.forward(self.xs[i])
    return self.xs[-1]

  def backward(self,e_y):
    self.e_ys[-1] = e_y
    for (i,l) in reversed(list(enumerate(self.layers))):
      self.e_ys[i] = l.backward(self.e_ys[i+1])
    return self.e_ys[0]

  def update_weights(self,print_weight_grads=False,update_weight=False,sign_reverse=False):
    for (i,l) in enumerate(self.layers):
      dW = l.update_weights(self.e_ys[i+1],update_weights=update_weight,sign_reverse=sign_reverse)
      if print_weight_grads:
        print("weight grads : ", i)
        print("dW: ", dW*2)
        print("weight grad: ",l.get_true_weight_grad())

  def save_model(self,savedir,logdir,losses,accs,test_accs):
      for i,l in enumerate(self.layers):
          l.save_layer(logdir,i)
      np.save(logdir +"/losses.npy",np.array(losses))
      np.save(logdir+"/accs.npy",np.array(accs))
      np.save(logdir+"/test_accs.npy",np.array(test_accs))
      subprocess.call(['rsync','--archive','--update','--compress','--progress',str(logdir) +"/",str(savedir)])
      print("Rsynced files from: " + str(logdir) + "/ " + " to" + str(savedir))
      now = datetime.now()
      current_time = str(now.strftime("%H:%M:%S"))
      subprocess.call(['echo','saved at time: ' + str(current_time)])

  def load_model(old_savedir):
      for (i,l) in enumerate(self.layers):
          l.load_layer(old_savedir,i)

  def test_accuracy(self,testset):
    accs = []
    for i,(inp, label) in enumerate(testset):
        pred_y = self.forward(inp.to(DEVICE))
        acc =accuracy(pred_y,onehot(label).to(DEVICE))
        accs.append(acc)
    return np.mean(np.array(accs)),accs

  def train(self, dataset,testset,n_epochs,n_inference_steps,savedir,logdir,old_savedir="",print_every=100,save_every=1):
    criterion = nn.CrossEntropyLoss()
    params = []
    for l in self.layers:
      params += l.parameters()
    
    print(" PARAMETERS: ", params)
    optimizer = opt.SGD(params, lr=0.001, momentum=0.9)
    if old_savedir != "None":
        self.load_model(old_savedir)
    accs = []
    losses = []
    test_accs =[]
    for n in range(n_epochs):
      print("Epoch: ",n)
      losslist = []
      for (i,(inp,label)) in enumerate(dataset):
        if (inp <0).any():
          print("Negative inputs")
          print(inp)
          stop
        out = self.forward(inp.to(DEVICE))
        label = label.to(DEVICE)
        optimizer.zero_grad()
        loss = criterion(out, label)
        loss.backward()
        optimizer.step()

      mean_acc, acclist = self.test_accuracy(dataset)
      accs.append(mean_acc)
      mean_loss = np.mean(np.array(losslist))
      losses.append(mean_loss)
      mean_test_acc, _ = self.test_accuracy(testset)
      test_accs.append(mean_test_acc)
      print("ACCURACY: ", mean_acc)
      print("TEST ACCURACY: ", mean_test_acc)
      print("SAVING MODEL")
      #self.save_model(logdir,savedir,losses,accs,test_accs)

class Args(object):
  def __init__(self):
    pass

if __name__ == '__main__':
    global DEVICE
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    DEVICE = "cpu"
    print("Initialized")
    #parsing arguments
    args = Args()
    args.logdir = "logs"
    args.savedir = "savedir"
    args.batch_size = 64
    args.learning_rate = 0.0005
    args.N_epochs = 100
    args.save_every = 1
    args.print_every = 10
    args.old_savedir = "None"
    args.n_inference_steps = 100
    args.inference_learning_rate = 0.1
    args.network_type = "pc"
    args.dataset = "cifar"
    args.loss_fn = "crossentropy"
    if args.savedir != "":
        subprocess.call(["mkdir","-p",str(args.savedir)])
    if args.logdir != "":
        subprocess.call(["mkdir","-p",str(args.logdir)])
    print("folders created")
    dataset,testset = get_cnn_dataset(args.dataset,args.batch_size)
    loss_fn, loss_fn_deriv = parse_loss_function(args.loss_fn)

    if args.dataset in ["cifar", "mnist","svhn"]:
        output_size = 10
    if args.dataset == "cifar100":
        output_size=100

    def onehot(x):
        z = torch.zeros([len(x),output_size])
        for i in range(len(x)):
            z[i,x[i]] = 1
        return z.float().to(DEVICE)
    #l1 = ConvLayer(32,3,6,64,5,args.learning_rate,relu,relu_deriv,device=DEVICE)
    #l2 = MaxPool(2,device=DEVICE)
    #l3 = ConvLayer(14,6,16,64,5,args.learning_rate,relu,relu_deriv,device=DEVICE)
    #l4 = ProjectionLayer((64,16,10,10),120,relu,relu_deriv,args.learning_rate,device=DEVICE)
    #l5 = FCLayer(120,84,64,args.learning_rate,relu,relu_deriv,device=DEVICE)
    #l6 = FCLayer(84,10,64,args.learning_rate,linear,linear_deriv,device=DEVICE)
    #layers =[l1,l2,l3,l4,l5,l6]
    l1 = ConvLayer(32,3,6,64,5,args.learning_rate,relu,relu_deriv,device=DEVICE)
    l2 = MaxPool(2,device=DEVICE)
    l3 = ConvLayer(14,6,16,64,5,args.learning_rate,relu,relu_deriv,device=DEVICE)
    l4 = ProjectionLayer((64,16,10,10),200,relu,relu_deriv,args.learning_rate,device=DEVICE)
    l5 = FCLayer(200,150,64,args.learning_rate,relu,relu_deriv,device=DEVICE)
    if args.loss_fn == "crossentropy":
      l6 = FCLayer(150,output_size,64,args.learning_rate,softmax,linear_deriv,device=DEVICE)
    else:
      l6 = FCLayer(150,output_size,64,args.learning_rate,linear,linear_deriv,device=DEVICE)
    layers =[l1,l2,l3,l4,l5,l6]
    #l1 = ConvLayer(32,3,20,64,4,args.learning_rate,tanh,tanh_deriv,device=DEVICE)
    #l2 = ConvLayer(29,20,50,64,5,args.learning_rate,tanh,tanh_deriv,device=DEVICE)
    #l3 = ConvLayer(25,50,50,64,5,args.learning_rate,tanh,tanh_deriv,stride=2,padding=1,device=DEVICE)
    #l4 = ConvLayer(12,50,5,64,3,args.learning_rate,tanh,tanh_deriv,stride=1,device=DEVICE)
    #l5 = ProjectionLayer((64,5,10,10),200,sigmoid,sigmoid_deriv,args.learning_rate,device=DEVICE)
    #l6 = FCLayer(200,100,64,args.learning_rate,linear,linear_deriv,device=DEVICE)
    #l7 = FCLayer(100,50,64,args.learning_rate,linear,linear_deriv,device=DEVICE)
    #l8 = FCLayer(50,10,64,args.learning_rate,linear,linear_deriv,device=DEVICE)
    #layers =[l1,l2,l3,l4,l5,l6,l7,l8]
    net = Backprop_CNN(layers,loss_fn, loss_fn_deriv, purely_positive=False, use_layer_norm=True)
    net.train(dataset[0:-2],testset[0:-2],args.N_epochs,args.n_inference_steps,args.savedir,args.logdir,args.old_savedir,args.save_every,args.print_every)
    # let's check if this thing actually works