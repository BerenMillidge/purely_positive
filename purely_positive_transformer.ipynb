{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "purely_positive_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y_ZTo-wnG-q"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeQrxEkIZqrh"
      },
      "source": [
        "# pytorch layers copied in from above\n",
        "import copy\n",
        "from typing import Optional, Any\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "import torch.nn.modules.module as Module\n",
        "from torch.nn.modules.activation import MultiheadAttention\n",
        "from torch.nn.modules.container import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import warnings\n",
        "from typing import Tuple, Optional\n",
        "from torch import Tensor\n",
        "from torch.nn.modules.linear import _LinearWithBias\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.init import constant_\n",
        "from torch.nn.init import xavier_normal_\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "    \n",
        "# modified slightly from the original pytorch implementation\n",
        "class MyTransformerEncoder(nn.Module):\n",
        "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
        "    Args:\n",
        "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
        "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
        "        norm: the layer normalization component (optional).\n",
        "    Examples::\n",
        "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        >>> src = torch.rand(10, 32, 512)\n",
        "        >>> out = transformer_encoder(src)\n",
        "    \"\"\"\n",
        "    __constants__ = ['norm']\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None,purely_positive=False,use_layer_norm=True,posfn = None):\n",
        "        super(MyTransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.purely_positive = purely_positive\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.posfn = posfn\n",
        "\n",
        "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        r\"\"\"Pass the input through the encoder layers in turn.\n",
        "        Args:\n",
        "            src: the sequence to the encoder (required).\n",
        "            mask: the mask for the src sequence (optional).\n",
        "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
        "        Shape:\n",
        "            see the docs in Transformer class.\n",
        "        \"\"\"\n",
        "        output = src\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "            if self.norm is not None and self.use_layer_norm:\n",
        "              output = self.norm(output)\n",
        "\n",
        "        #if self.norm is not None:\n",
        "        #    output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MyTransformerEncoderLayer(nn.Module):\n",
        "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
        "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
        "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
        "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
        "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
        "    in a different way during application.\n",
        "    Args:\n",
        "        d_model: the number of expected features in the input (required).\n",
        "        nhead: the number of heads in the multiheadattention models (required).\n",
        "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "    Examples::\n",
        "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "        >>> src = torch.rand(10, 32, 512)\n",
        "        >>> out = encoder_layer(src)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",purely_positive=False,posfn=None, act_fn = F.relu):\n",
        "        super(MyTransformerEncoderLayer, self).__init__()\n",
        "        self.posfn = posfn\n",
        "        self.self_attn = MyMultiheadAttention(d_model, nhead, dropout=dropout,purely_positive=purely_positive,posfn = self.posfn)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = MyLinear(d_model, dim_feedforward,purely_positive=purely_positive,posfn = self.posfn)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = MyLinear(dim_feedforward, d_model,purely_positive=purely_positive,posfn = self.posfn)\n",
        "\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.activation = act_fn\n",
        "        self.purely_positive = purely_positive\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(MyTransformerEncoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "        Args:\n",
        "            src: the sequence to the encoder layer (required).\n",
        "            src_mask: the mask for the src sequence (optional).\n",
        "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
        "        Shape:\n",
        "            see the docs in Transformer class.\n",
        "        \"\"\"\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.activation(self.norm2(src))\n",
        "        \n",
        "        return src\n",
        "\n",
        "\n",
        "class MyLinear(nn.Module):\n",
        "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
        "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
        "    Args:\n",
        "        in_features: size of each input sample\n",
        "        out_features: size of each output sample\n",
        "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
        "            Default: ``True``\n",
        "    Shape:\n",
        "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
        "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
        "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
        "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
        "    Attributes:\n",
        "        weight: the learnable weights of the module of shape\n",
        "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
        "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
        "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
        "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
        "                If :attr:`bias` is ``True``, the values are initialized from\n",
        "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
        "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
        "    Examples::\n",
        "        >>> m = nn.Linear(20, 30)\n",
        "        >>> input = torch.randn(128, 20)\n",
        "        >>> output = m(input)\n",
        "        >>> print(output.size())\n",
        "        torch.Size([128, 30])\n",
        "    \"\"\"\n",
        "    __constants__ = ['in_features', 'out_features']\n",
        "    in_features: int\n",
        "    out_features: int\n",
        "    weight: Tensor\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True,purely_positive=False,posfn=None) -> None:\n",
        "        super(MyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        self.purely_positive = purely_positive\n",
        "        self.posfn = posfn\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if self.purely_positive:\n",
        "          return F.linear(input, self.posfn(self.weight), self.posfn(self.bias))\n",
        "        else:\n",
        "          return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )\n",
        "\n",
        "\n",
        "class MyMultiheadAttention(nn.Module):\n",
        "    r\"\"\"Allows the model to jointly attend to information\n",
        "    from different representation subspaces.\n",
        "    See reference: Attention Is All You Need\n",
        "    .. math::\n",
        "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
        "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
        "    Args:\n",
        "        embed_dim: total dimension of the model.\n",
        "        num_heads: parallel attention heads.\n",
        "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
        "        bias: add bias as module parameter. Default: True.\n",
        "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
        "        add_zero_attn: add a new batch of zeros to the key and\n",
        "                       value sequences at dim=1.\n",
        "        kdim: total number of features in key. Default: None.\n",
        "        vdim: total number of features in value. Default: None.\n",
        "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
        "        query, key, and value have the same number of features.\n",
        "    Examples::\n",
        "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "    \"\"\"\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None,purely_positive=False,posfn = None):\n",
        "        super(MyMultiheadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "        self.purely_positive = purely_positive\n",
        "        self.posfn = posfn\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        if self._qkv_same_embed_dim is False:\n",
        "            self.q_proj_weight = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
        "            self.k_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.kdim))\n",
        "            self.v_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.vdim))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            self.in_proj_weight = nn.Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        self.out_proj = _LinearWithBias(embed_dim, embed_dim)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
        "            self.bias_v = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            xavier_uniform_(self.q_proj_weight)\n",
        "            xavier_uniform_(self.k_proj_weight)\n",
        "            xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.)\n",
        "            constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super(MyMultiheadAttention, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None,\n",
        "                need_weights=True, attn_mask=None):\n",
        "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
        "        r\"\"\"\n",
        "    Args:\n",
        "        query, key, value: map a query and a set of key-value pairs to an output.\n",
        "            See \"Attention Is All You Need\" for more details.\n",
        "        key_padding_mask: if provided, specified padding elements in the key will\n",
        "            be ignored by the attention. When given a binary mask and a value is True,\n",
        "            the corresponding value on the attention layer will be ignored. When given\n",
        "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
        "            layer will be ignored\n",
        "        need_weights: output attn_output_weights.\n",
        "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
        "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
        "    Shape:\n",
        "        - Inputs:\n",
        "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
        "          the embedding dimension.\n",
        "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
        "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
        "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
        "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
        "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
        "          3D mask :math:`(N*\\text{num_heads}, L, S)` where N is the batch size, L is the target sequence length,\n",
        "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
        "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
        "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
        "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
        "          is provided, it will be added to the attention weight.\n",
        "        - Outputs:\n",
        "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
        "          E is the embedding dimension.\n",
        "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
        "          L is the target sequence length, S is the source sequence length.\n",
        "        \"\"\"\n",
        "        if not self._qkv_same_embed_dim:\n",
        "          if self.purely_positive:\n",
        "            return F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.posfn(self.in_proj_weight), self.posfn(self.in_proj_bias),\n",
        "                self.posfn(self.bias_k), self.posfn(self.bias_v), self.add_zero_attn,\n",
        "                self.dropout, self.posfn(self.out_proj.weight), self.posfn(self.out_proj.bias),\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.posfn(self.q_proj_weight), k_proj_weight=self.posfn(self.k_proj_weight),\n",
        "                v_proj_weight=self.posfn(self.v_proj_weight))\n",
        "          else:\n",
        "            return F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight)\n",
        "        else:\n",
        "          if self.purely_positive:\n",
        "            return F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.posfn(self.in_proj_weight), self.posfn(self.in_proj_bias),\n",
        "                self.posfn(self.bias_k), self.posfn(self.bias_v),self.add_zero_attn,\n",
        "                self.dropout, self.posfn(self.out_proj.weight), self.posfn(self.out_proj.bias),\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask)\n",
        "          else:\n",
        "            return F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFKvyzo1-GM5"
      },
      "source": [
        "# the goal here is to work through https://pytorch.org/tutorials/beginner/transformer_tutorial.html -- this tutorial to build my pytorch transformer and then to test to see if it can work in the purely positive phase\n",
        "# okay let's start copying this across although I disagree with it. I want a good dataset which will be SHORT FOR TRAINING so I can check it works. Let's try pytorch transformer tutorial first\n",
        "\n",
        "\n",
        "#from torch.nn import TransformerEncoder, TransformerEncoderLayer # could just copy paste these into the script\n",
        "# and edit them lightly to allow for positivity\n",
        "# we can softmax the embeddings to ensure input positivity\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, purely_positive = False,posfn=None,act_fn=F.relu):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.model_type=\"Transformer\"\n",
        "    self.purely_positive = purely_positive\n",
        "    self.posfn = posfn\n",
        "    self.act_fn = act_fn\n",
        "    self.pos_encoder = PositionalEncoding(ninp, dropout, purely_positive = self.purely_positive,posfn=self.posfn)\n",
        "    encoder_layers = MyTransformerEncoderLayer(ninp, nhead, nhid, dropout,purely_positive = self.purely_positive,posfn = self.posfn,act_fn = self.act_fn)\n",
        "    self.transformer_encoder = MyTransformerEncoder(encoder_layers, nlayers, purely_positive = self.purely_positive,posfn = self.posfn)\n",
        "    self.encoder = nn.Embedding(ntoken, ninp)\n",
        "    self.ninp = ninp\n",
        "    self.decoder = MyLinear(ninp, ntoken,purely_positive = self.purely_positive,posfn = self.posfn)\n",
        "    self.init_weights()\n",
        "\n",
        "  def generate_square_subsequent_mask(self, sz):\n",
        "    mask = (torch.triu(torch.ones(sz,sz)) == 1).transpose(0,1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask \n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "# positional encoding module\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout=0.1, max_len=5000,purely_positive=False,posfn=None):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0)/d_model))\n",
        "    pe[:,0::2] = torch.sin(position * div_term)\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0,1)\n",
        "    self.register_buffer('pe',pe)\n",
        "    self.purely_positive = purely_positive\n",
        "    self.posfn = posfn\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0),:]\n",
        "    if self.purely_positive:\n",
        "      return self.posfn(self.dropout(x))\n",
        "    else:\n",
        "      return self.dropout(x)\n",
        "\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOstFesCnEVf"
      },
      "source": [
        "# Text Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mGsNrPiTLB6"
      },
      "source": [
        "\n",
        "def extract_archive(from_path, to_path=None, overwrite=False):\n",
        "    \"\"\"Extract archive.\n",
        "    Arguments:\n",
        "        from_path: the path of the archive.\n",
        "        to_path: the root path of the extracted files (directory of from_path)\n",
        "        overwrite: overwrite existing files (False)\n",
        "    Returns:\n",
        "        List of paths to extracted files even if not overwritten.\n",
        "    Examples:\n",
        "        >>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\n",
        "        >>> from_path = './validation.tar.gz'\n",
        "        >>> to_path = './'\n",
        "        >>> torchtext.utils.download_from_url(url, from_path)\n",
        "        >>> torchtext.utils.extract_archive(from_path, to_path)\n",
        "        >>> ['.data/val.de', '.data/val.en']\n",
        "        >>> torchtext.utils.download_from_url(url, from_path)\n",
        "        >>> torchtext.utils.extract_archive(from_path, to_path)\n",
        "        >>> ['.data/val.de', '.data/val.en']\n",
        "    \"\"\"\n",
        "\n",
        "    if to_path is None:\n",
        "        to_path = os.path.dirname(from_path)\n",
        "\n",
        "    if from_path.endswith(('.tar.gz', '.tgz')):\n",
        "        logging.info('Opening tar file {}.'.format(from_path))\n",
        "        with tarfile.open(from_path, 'r') as tar:\n",
        "            files = []\n",
        "            for file_ in tar:\n",
        "                file_path = os.path.join(to_path, file_.name)\n",
        "                if file_.isfile():\n",
        "                    files.append(file_path)\n",
        "                    if os.path.exists(file_path):\n",
        "                        logging.info('{} already extracted.'.format(file_path))\n",
        "                        if not overwrite:\n",
        "                            continue\n",
        "                tar.extract(file_, to_path)\n",
        "            return files\n",
        "\n",
        "    elif from_path.endswith('.zip'):\n",
        "        assert zipfile.is_zipfile(from_path), from_path\n",
        "        logging.info('Opening zip file {}.'.format(from_path))\n",
        "        with zipfile.ZipFile(from_path, 'r') as zfile:\n",
        "            files = []\n",
        "            for file_ in zfile.namelist():\n",
        "                file_path = os.path.join(to_path, file_)\n",
        "                files.append(file_path)\n",
        "                if os.path.exists(file_path):\n",
        "                    logging.info('{} already extracted.'.format(file_path))\n",
        "                    if not overwrite:\n",
        "                        continue\n",
        "                zfile.extract(file_, to_path)\n",
        "        files = [f for f in files if os.path.isfile(f)]\n",
        "        return files\n",
        "\n",
        "    elif from_path.endswith('.gz'):\n",
        "        default_block_size = 65536\n",
        "        filename = from_path[:-3]\n",
        "        files = [filename]\n",
        "        with gzip.open(from_path, 'rb') as gzfile, \\\n",
        "                open(filename, 'wb') as d_file:\n",
        "            while True:\n",
        "                block = gzfile.read(default_block_size)\n",
        "                if not block:\n",
        "                    break\n",
        "                else:\n",
        "                    d_file.write(block)\n",
        "            d_file.write(block)\n",
        "        return files\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            \"We currently only support tar.gz, .tgz, .gz and zip achives.\")\n",
        "\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "import logging\n",
        "import os\n",
        "import zipfile\n",
        "import gzip\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import tarfile\n",
        "\n",
        "from torchtext.utils import reporthook\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
        "    Attributes:\n",
        "        freqs: A collections.Counter object holding the frequencies of tokens\n",
        "            in the data used to build the Vocab.\n",
        "        stoi: A collections.defaultdict instance mapping token strings to\n",
        "            numerical identifiers.\n",
        "        itos: A list of token strings indexed by their numerical identifiers.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO (@mttk): Populate classs with default values of special symbols\n",
        "    UNK = '<unk>'\n",
        "\n",
        "    def __init__(self, counter, max_size=None, min_freq=1, specials=('<unk>', '<pad>'),\n",
        "                 vectors=None, unk_init=None, vectors_cache=None, specials_first=True):\n",
        "        \"\"\"Create a Vocab object from a collections.Counter.\n",
        "        Arguments:\n",
        "            counter: collections.Counter object holding the frequencies of\n",
        "                each value found in the data.\n",
        "            max_size: The maximum size of the vocabulary, or None for no\n",
        "                maximum. Default: None.\n",
        "            min_freq: The minimum frequency needed to include a token in the\n",
        "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
        "            specials: The list of special tokens (e.g., padding or eos) that\n",
        "                will be prepended to the vocabulary. Default: ['<unk'>, '<pad>']\n",
        "            vectors: One of either the available pretrained vectors\n",
        "                or custom pretrained vectors (see Vocab.load_vectors);\n",
        "                or a list of aforementioned vectors\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and\n",
        "                returns a Tensor of the same size. Default: 'torch.zeros'\n",
        "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
        "            specials_first: Whether to add special tokens into the vocabulary at first.\n",
        "                If it is False, they are added into the vocabulary at last.\n",
        "                Default: True.\n",
        "        \"\"\"\n",
        "        self.freqs = counter\n",
        "        counter = counter.copy()\n",
        "        min_freq = max(min_freq, 1)\n",
        "\n",
        "        self.itos = list()\n",
        "        self.unk_index = None\n",
        "        if specials_first:\n",
        "            self.itos = list(specials)\n",
        "            # only extend max size if specials are prepended\n",
        "            max_size = None if max_size is None else max_size + len(specials)\n",
        "\n",
        "        # frequencies of special tokens are not counted when building vocabulary\n",
        "        # in frequency order\n",
        "        for tok in specials:\n",
        "            del counter[tok]\n",
        "\n",
        "        # sort by frequency, then alphabetically\n",
        "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
        "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        for word, freq in words_and_frequencies:\n",
        "            if freq < min_freq or len(self.itos) == max_size:\n",
        "                break\n",
        "            self.itos.append(word)\n",
        "\n",
        "        if Vocab.UNK in specials:  # hard-coded for now\n",
        "            unk_index = specials.index(Vocab.UNK)  # position in list\n",
        "            # account for ordering of specials, set variable\n",
        "            self.unk_index = unk_index if specials_first else len(self.itos) + unk_index\n",
        "            self.stoi = defaultdict(self._default_unk_index)\n",
        "        else:\n",
        "            self.stoi = defaultdict()\n",
        "\n",
        "        if not specials_first:\n",
        "            self.itos.extend(list(specials))\n",
        "\n",
        "        # stoi is simply a reverse dict for itos\n",
        "        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})\n",
        "\n",
        "        self.vectors = None\n",
        "        if vectors is not None:\n",
        "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
        "        else:\n",
        "            assert unk_init is None and vectors_cache is None\n",
        "\n",
        "    def _default_unk_index(self):\n",
        "        return self.unk_index\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        return self.stoi.get(token, self.stoi.get(Vocab.UNK))\n",
        "\n",
        "    def __getstate__(self):\n",
        "        # avoid picking defaultdict\n",
        "        attrs = dict(self.__dict__)\n",
        "        # cast to regular dict\n",
        "        attrs['stoi'] = dict(self.stoi)\n",
        "        return attrs\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if state.get(\"unk_index\", None) is None:\n",
        "            stoi = defaultdict()\n",
        "        else:\n",
        "            stoi = defaultdict(self._default_unk_index)\n",
        "        stoi.update(state['stoi'])\n",
        "        state['stoi'] = stoi\n",
        "        self.__dict__.update(state)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if self.freqs != other.freqs:\n",
        "            return False\n",
        "        if self.stoi != other.stoi:\n",
        "            return False\n",
        "        if self.itos != other.itos:\n",
        "            return False\n",
        "        if self.vectors != other.vectors:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def lookup_indices(self, tokens):\n",
        "        indices = [self.__getitem__(token) for token in tokens]\n",
        "        return indices\n",
        "\n",
        "    def extend(self, v, sort=False):\n",
        "        words = sorted(v.itos) if sort else v.itos\n",
        "        for w in words:\n",
        "            if w not in self.stoi:\n",
        "                self.itos.append(w)\n",
        "                self.stoi[w] = len(self.itos) - 1\n",
        "\n",
        "    def load_vectors(self, vectors, **kwargs):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            vectors: one of or a list containing instantiations of the\n",
        "                GloVe, CharNGram, or Vectors classes. Alternatively, one\n",
        "                of or a list of available pretrained vectors:\n",
        "                charngram.100d\n",
        "                fasttext.en.300d\n",
        "                fasttext.simple.300d\n",
        "                glove.42B.300d\n",
        "                glove.840B.300d\n",
        "                glove.twitter.27B.25d\n",
        "                glove.twitter.27B.50d\n",
        "                glove.twitter.27B.100d\n",
        "                glove.twitter.27B.200d\n",
        "                glove.6B.50d\n",
        "                glove.6B.100d\n",
        "                glove.6B.200d\n",
        "                glove.6B.300d\n",
        "            Remaining keyword arguments: Passed to the constructor of Vectors classes.\n",
        "        \"\"\"\n",
        "        if not isinstance(vectors, list):\n",
        "            vectors = [vectors]\n",
        "        for idx, vector in enumerate(vectors):\n",
        "            if isinstance(vector, str):\n",
        "                # Convert the string pretrained vector identifier\n",
        "                # to a Vectors object\n",
        "                if vector not in pretrained_aliases:\n",
        "                    raise ValueError(\n",
        "                        \"Got string input vector {}, but allowed pretrained \"\n",
        "                        \"vectors are {}\".format(\n",
        "                            vector, list(pretrained_aliases.keys())))\n",
        "                vectors[idx] = pretrained_aliases[vector](**kwargs)\n",
        "            elif not isinstance(vector, Vectors):\n",
        "                raise ValueError(\n",
        "                    \"Got input vectors of type {}, expected str or \"\n",
        "                    \"Vectors object\".format(type(vector)))\n",
        "\n",
        "        tot_dim = sum(v.dim for v in vectors)\n",
        "        self.vectors = torch.Tensor(len(self), tot_dim)\n",
        "        for i, token in enumerate(self.itos):\n",
        "            start_dim = 0\n",
        "            for v in vectors:\n",
        "                end_dim = start_dim + v.dim\n",
        "                self.vectors[i][start_dim:end_dim] = v[token.strip()]\n",
        "                start_dim = end_dim\n",
        "            assert(start_dim == tot_dim)\n",
        "\n",
        "    def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n",
        "        \"\"\"\n",
        "        Set the vectors for the Vocab instance from a collection of Tensors.\n",
        "        Arguments:\n",
        "            stoi: A dictionary of string to the index of the associated vector\n",
        "                in the `vectors` input argument.\n",
        "            vectors: An indexed iterable (or other structure supporting __getitem__) that\n",
        "                given an input index, returns a FloatTensor representing the vector\n",
        "                for the token associated with the index. For example,\n",
        "                vector[stoi[\"string\"]] should return the vector for \"string\".\n",
        "            dim: The dimensionality of the vectors.\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and\n",
        "                returns a Tensor of the same size. Default: 'torch.zeros'\n",
        "        \"\"\"\n",
        "        self.vectors = torch.Tensor(len(self), dim)\n",
        "        for i, token in enumerate(self.itos):\n",
        "            wv_index = stoi.get(token, None)\n",
        "            if wv_index is not None:\n",
        "                self.vectors[i] = vectors[wv_index]\n",
        "            else:\n",
        "                self.vectors[i] = unk_init(self.vectors[i])\n",
        "\n",
        "\n",
        "class SubwordVocab(Vocab):\n",
        "\n",
        "    def __init__(self, counter, max_size=None, specials=('<pad>'),\n",
        "                 vectors=None, unk_init=torch.Tensor.zero_):\n",
        "        \"\"\"Create a revtok subword vocabulary from a collections.Counter.\n",
        "        Arguments:\n",
        "            counter: collections.Counter object holding the frequencies of\n",
        "                each word found in the data.\n",
        "            max_size: The maximum size of the subword vocabulary, or None for no\n",
        "                maximum. Default: None.\n",
        "            specials: The list of special tokens (e.g., padding or eos) that\n",
        "                will be prepended to the vocabulary in addition to an <unk>\n",
        "                token.\n",
        "            vectors: One of either the available pretrained vectors\n",
        "                or custom pretrained vectors (see Vocab.load_vectors);\n",
        "                or a list of aforementioned vectors\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and\n",
        "                returns a Tensor of the same size. Default: 'torch.zeros\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import revtok\n",
        "        except ImportError:\n",
        "            print(\"Please install revtok.\")\n",
        "            raise\n",
        "\n",
        "        # Hardcode unk_index as subword_vocab has no specials_first argument\n",
        "        self.unk_index = (specials.index(SubwordVocab.UNK)\n",
        "                          if SubwordVocab.UNK in specials else None)\n",
        "\n",
        "        if self.unk_index is None:\n",
        "            self.stoi = defaultdict()\n",
        "        else:\n",
        "            self.stoi = defaultdict(self._default_unk_index)\n",
        "\n",
        "        self.stoi.update({tok: i for i, tok in enumerate(specials)})\n",
        "        self.itos = specials.copy()\n",
        "\n",
        "        self.segment = revtok.SubwordSegmenter(counter, max_size)\n",
        "\n",
        "        max_size = None if max_size is None else max_size + len(self.itos)\n",
        "\n",
        "        # sort by frequency/entropy, then alphabetically\n",
        "        toks = sorted(self.segment.vocab.items(),\n",
        "                      key=lambda tup: (len(tup[0]) != 1, -tup[1], tup[0]))\n",
        "\n",
        "        for tok, _ in toks:\n",
        "            if len(self.itos) == max_size:\n",
        "                break\n",
        "            self.itos.append(tok)\n",
        "            self.stoi[tok] = len(self.itos) - 1\n",
        "\n",
        "        if vectors is not None:\n",
        "            self.load_vectors(vectors, unk_init=unk_init)\n",
        "\n",
        "\n",
        "def _infer_shape(f):\n",
        "    num_lines, vector_dim = 0, None\n",
        "    for line in f:\n",
        "        if vector_dim is None:\n",
        "            row = line.rstrip().split(b\" \")\n",
        "            vector = row[1:]\n",
        "            # Assuming word, [vector] format\n",
        "            if len(vector) > 2:\n",
        "                # The header present in some (w2v) formats contains two elements.\n",
        "                vector_dim = len(vector)\n",
        "                num_lines += 1  # First element read\n",
        "        else:\n",
        "            num_lines += 1\n",
        "    f.seek(0)\n",
        "    return num_lines, vector_dim\n",
        "\n",
        "\n",
        "class Vectors(object):\n",
        "\n",
        "    def __init__(self, name, cache=None,\n",
        "                 url=None, unk_init=None, max_vectors=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            name: name of the file that contains the vectors\n",
        "            cache: directory for cached vectors\n",
        "            url: url for download if vectors not found in cache\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\n",
        "            max_vectors (int): this can be used to limit the number of\n",
        "                pre-trained vectors loaded.\n",
        "                Most pre-trained vector sets are sorted\n",
        "                in the descending order of word frequency.\n",
        "                Thus, in situations where the entire set doesn't fit in memory,\n",
        "                or is not needed for another reason, passing `max_vectors`\n",
        "                can limit the size of the loaded set.\n",
        "        \"\"\"\n",
        "\n",
        "        cache = '.vector_cache' if cache is None else cache\n",
        "        self.itos = None\n",
        "        self.stoi = None\n",
        "        self.vectors = None\n",
        "        self.dim = None\n",
        "        self.unk_init = torch.Tensor.zero_ if unk_init is None else unk_init\n",
        "        self.cache(name, cache, url=url, max_vectors=max_vectors)\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        if token in self.stoi:\n",
        "            return self.vectors[self.stoi[token]]\n",
        "        else:\n",
        "            return self.unk_init(torch.Tensor(self.dim))\n",
        "\n",
        "    def cache(self, name, cache, url=None, max_vectors=None):\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        if os.path.isfile(name):\n",
        "            path = name\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = os.path.join(cache, os.path.basename(name)) + file_suffix\n",
        "        else:\n",
        "            path = os.path.join(cache, name)\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = path + file_suffix\n",
        "\n",
        "        if not os.path.isfile(path_pt):\n",
        "            if not os.path.isfile(path) and url:\n",
        "                logger.info('Downloading vectors from {}'.format(url))\n",
        "                if not os.path.exists(cache):\n",
        "                    os.makedirs(cache)\n",
        "                dest = os.path.join(cache, os.path.basename(url))\n",
        "                if not os.path.isfile(dest):\n",
        "                    with tqdm(unit='B', unit_scale=True, miniters=1, desc=dest) as t:\n",
        "                        try:\n",
        "                            urlretrieve(url, dest, reporthook=reporthook(t))\n",
        "                        except KeyboardInterrupt as e:  # remove the partial zip file\n",
        "                            os.remove(dest)\n",
        "                            raise e\n",
        "                logger.info('Extracting vectors into {}'.format(cache))\n",
        "                ext = os.path.splitext(dest)[1][1:]\n",
        "                if ext == 'zip':\n",
        "                    with zipfile.ZipFile(dest, \"r\") as zf:\n",
        "                        zf.extractall(cache)\n",
        "                elif ext == 'gz':\n",
        "                    if dest.endswith('.tar.gz'):\n",
        "                        with tarfile.open(dest, 'r:gz') as tar:\n",
        "                            tar.extractall(path=cache)\n",
        "            if not os.path.isfile(path):\n",
        "                raise RuntimeError('no vectors found at {}'.format(path))\n",
        "\n",
        "            logger.info(\"Loading vectors from {}\".format(path))\n",
        "            ext = os.path.splitext(path)[1][1:]\n",
        "            if ext == 'gz':\n",
        "                open_file = gzip.open\n",
        "            else:\n",
        "                open_file = open\n",
        "\n",
        "            vectors_loaded = 0\n",
        "            with open_file(path, 'rb') as f:\n",
        "                num_lines, dim = _infer_shape(f)\n",
        "                if not max_vectors or max_vectors > num_lines:\n",
        "                    max_vectors = num_lines\n",
        "\n",
        "                itos, vectors, dim = [], torch.zeros((max_vectors, dim)), None\n",
        "\n",
        "                for line in tqdm(f, total=max_vectors):\n",
        "                    # Explicitly splitting on \" \" is important, so we don't\n",
        "                    # get rid of Unicode non-breaking spaces in the vectors.\n",
        "                    entries = line.rstrip().split(b\" \")\n",
        "\n",
        "                    word, entries = entries[0], entries[1:]\n",
        "                    if dim is None and len(entries) > 1:\n",
        "                        dim = len(entries)\n",
        "                    elif len(entries) == 1:\n",
        "                        logger.warning(\"Skipping token {} with 1-dimensional \"\n",
        "                                       \"vector {}; likely a header\".format(word, entries))\n",
        "                        continue\n",
        "                    elif dim != len(entries):\n",
        "                        raise RuntimeError(\n",
        "                            \"Vector for token {} has {} dimensions, but previously \"\n",
        "                            \"read vectors have {} dimensions. All vectors must have \"\n",
        "                            \"the same number of dimensions.\".format(word, len(entries),\n",
        "                                                                    dim))\n",
        "\n",
        "                    try:\n",
        "                        if isinstance(word, bytes):\n",
        "                            word = word.decode('utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        logger.info(\"Skipping non-UTF8 token {}\".format(repr(word)))\n",
        "                        continue\n",
        "\n",
        "                    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\n",
        "                    vectors_loaded += 1\n",
        "                    itos.append(word)\n",
        "\n",
        "                    if vectors_loaded == max_vectors:\n",
        "                        break\n",
        "\n",
        "            self.itos = itos\n",
        "            self.stoi = {word: i for i, word in enumerate(itos)}\n",
        "            self.vectors = torch.Tensor(vectors).view(-1, dim)\n",
        "            self.dim = dim\n",
        "            logger.info('Saving vectors to {}'.format(path_pt))\n",
        "            if not os.path.exists(cache):\n",
        "                os.makedirs(cache)\n",
        "            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n",
        "        else:\n",
        "            logger.info('Loading vectors from {}'.format(path_pt))\n",
        "            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vectors)\n",
        "\n",
        "    def get_vecs_by_tokens(self, tokens, lower_case_backup=False):\n",
        "        \"\"\"Look up embedding vectors of tokens.\n",
        "        Arguments:\n",
        "            tokens: a token or a list of tokens. if `tokens` is a string,\n",
        "                returns a 1-D tensor of shape `self.dim`; if `tokens` is a\n",
        "                list of strings, returns a 2-D tensor of shape=(len(tokens),\n",
        "                self.dim).\n",
        "            lower_case_backup : Whether to look up the token in the lower case.\n",
        "                If False, each token in the original case will be looked up;\n",
        "                if True, each token in the original case will be looked up first,\n",
        "                if not found in the keys of the property `stoi`, the token in the\n",
        "                lower case will be looked up. Default: False.\n",
        "        Examples:\n",
        "            >>> examples = ['chip', 'baby', 'Beautiful']\n",
        "            >>> vec = text.vocab.GloVe(name='6B', dim=50)\n",
        "            >>> ret = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
        "        \"\"\"\n",
        "        to_reduce = False\n",
        "\n",
        "        if not isinstance(tokens, list):\n",
        "            tokens = [tokens]\n",
        "            to_reduce = True\n",
        "\n",
        "        if not lower_case_backup:\n",
        "            indices = [self[token] for token in tokens]\n",
        "        else:\n",
        "            indices = [self[token] if token in self.stoi\n",
        "                       else self[token.lower()]\n",
        "                       for token in tokens]\n",
        "\n",
        "        vecs = torch.stack(indices)\n",
        "        return vecs[0] if to_reduce else vecs\n",
        "\n",
        "\n",
        "class GloVe(Vectors):\n",
        "    url = {\n",
        "        '42B': 'http://nlp.stanford.edu/data/glove.42B.300d.zip',\n",
        "        '840B': 'http://nlp.stanford.edu/data/glove.840B.300d.zip',\n",
        "        'twitter.27B': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip',\n",
        "        '6B': 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
        "    }\n",
        "\n",
        "    def __init__(self, name='840B', dim=300, **kwargs):\n",
        "        url = self.url[name]\n",
        "        name = 'glove.{}.{}d.txt'.format(name, str(dim))\n",
        "        super(GloVe, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "\n",
        "class FastText(Vectors):\n",
        "\n",
        "    url_base = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec'\n",
        "\n",
        "    def __init__(self, language=\"en\", **kwargs):\n",
        "        url = self.url_base.format(language)\n",
        "        name = os.path.basename(url)\n",
        "        super(FastText, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "\n",
        "class CharNGram(Vectors):\n",
        "\n",
        "    name = 'charNgram.txt'\n",
        "    url = ('http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/'\n",
        "           'jmt_pre-trained_embeddings.tar.gz')\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CharNGram, self).__init__(self.name, url=self.url, **kwargs)\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        vector = torch.Tensor(1, self.dim).zero_()\n",
        "        if token == \"<unk>\":\n",
        "            return self.unk_init(vector)\n",
        "        chars = ['#BEGIN#'] + list(token) + ['#END#']\n",
        "        num_vectors = 0\n",
        "        for n in [2, 3, 4]:\n",
        "            end = len(chars) - n + 1\n",
        "            grams = [chars[i:(i + n)] for i in range(end)]\n",
        "            for gram in grams:\n",
        "                gram_key = '{}gram-{}'.format(n, ''.join(gram))\n",
        "                if gram_key in self.stoi:\n",
        "                    vector += self.vectors[self.stoi[gram_key]]\n",
        "                    num_vectors += 1\n",
        "        if num_vectors > 0:\n",
        "            vector /= num_vectors\n",
        "        else:\n",
        "            vector = self.unk_init(vector)\n",
        "        return vector\n",
        "\n",
        "\n",
        "pretrained_aliases = {\n",
        "    \"charngram.100d\": partial(CharNGram),\n",
        "    \"fasttext.en.300d\": partial(FastText, language=\"en\"),\n",
        "    \"fasttext.simple.300d\": partial(FastText, language=\"simple\"),\n",
        "    \"glove.42B.300d\": partial(GloVe, name=\"42B\", dim=\"300\"),\n",
        "    \"glove.840B.300d\": partial(GloVe, name=\"840B\", dim=\"300\"),\n",
        "    \"glove.twitter.27B.25d\": partial(GloVe, name=\"twitter.27B\", dim=\"25\"),\n",
        "    \"glove.twitter.27B.50d\": partial(GloVe, name=\"twitter.27B\", dim=\"50\"),\n",
        "    \"glove.twitter.27B.100d\": partial(GloVe, name=\"twitter.27B\", dim=\"100\"),\n",
        "    \"glove.twitter.27B.200d\": partial(GloVe, name=\"twitter.27B\", dim=\"200\"),\n",
        "    \"glove.6B.50d\": partial(GloVe, name=\"6B\", dim=\"50\"),\n",
        "    \"glove.6B.100d\": partial(GloVe, name=\"6B\", dim=\"100\"),\n",
        "    \"glove.6B.200d\": partial(GloVe, name=\"6B\", dim=\"200\"),\n",
        "    \"glove.6B.300d\": partial(GloVe, name=\"6B\", dim=\"300\")\n",
        "}\n",
        "\"\"\"Mapping from string name to factory function\"\"\"\n",
        "\n",
        "\n",
        "def build_vocab_from_iterator(iterator, num_lines=None):\n",
        "    \"\"\"\n",
        "    Build a Vocab from an iterator.\n",
        "    Arguments:\n",
        "        iterator: Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
        "        num_lines: The expected number of elements returned by the iterator.\n",
        "            (Default: None)\n",
        "            Optionally, if known, the expected number of elements can be passed to\n",
        "            this factory function for improved progress reporting.\n",
        "    \"\"\"\n",
        "\n",
        "    counter = Counter()\n",
        "    with tqdm(unit_scale=0, unit='lines', total=num_lines) as t:\n",
        "        for tokens in iterator:\n",
        "            counter.update(tokens)\n",
        "            t.update(1)\n",
        "    word_vocab = Vocab(counter)\n",
        "    return word_vocab"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWVjE8FaU-Ey"
      },
      "source": [
        "import requests\n",
        "import csv\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tarfile\n",
        "import logging\n",
        "import re\n",
        "import sys\n",
        "import zipfile\n",
        "import gzip\n",
        "\n",
        "def download_from_url(url, path=None, root='.data', overwrite=False, hash_value=None,\n",
        "                      hash_type=\"sha256\"):\n",
        "    \"\"\"Download file, with logic (from tensor2tensor) for Google Drive. Returns\n",
        "    the path to the downloaded file.\n",
        "    Arguments:\n",
        "        url: the url of the file from URL header. (None)\n",
        "        root: download folder used to store the file in (.data)\n",
        "        overwrite: overwrite existing files (False)\n",
        "        hash_value (str, optional): hash for url (Default: ``None``).\n",
        "        hash_type (str, optional): hash type, among \"sha256\" and \"md5\" (Default: ``\"sha256\"``).\n",
        "    Examples:\n",
        "        >>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\n",
        "        >>> torchtext.utils.download_from_url(url)\n",
        "        >>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\n",
        "        >>> torchtext.utils.download_from_url(url)\n",
        "        >>> '.data/validation.tar.gz'\n",
        "    \"\"\"\n",
        "    def _check_hash(path):\n",
        "        if hash_value:\n",
        "            with open(path, \"rb\") as file_obj:\n",
        "                if not validate_file(file_obj, hash_value, hash_type):\n",
        "                    raise RuntimeError(\"The hash of {} does not match. Delete the file manually and retry.\".format(path))\n",
        "\n",
        "    def _process_response(r, root, filename):\n",
        "        chunk_size = 16 * 1024\n",
        "        total_size = int(r.headers.get('Content-length', 0))\n",
        "        if filename is None:\n",
        "            d = r.headers['content-disposition']\n",
        "            filename = re.findall(\"filename=\\\"(.+)\\\"\", d)\n",
        "            if filename is None:\n",
        "                raise RuntimeError(\"Filename could not be autodetected\")\n",
        "            filename = filename[0]\n",
        "        path = os.path.join(root, filename)\n",
        "        if os.path.exists(path):\n",
        "            logging.info('File %s already exists.' % path)\n",
        "            if not overwrite:\n",
        "                _check_hash(path)\n",
        "                return path\n",
        "            logging.info('Overwriting file %s.' % path)\n",
        "        logging.info('Downloading file {} to {}.'.format(filename, path))\n",
        "        with open(path, \"wb\") as file:\n",
        "            with tqdm(total=total_size, unit='B',\n",
        "                      unit_scale=1, desc=path.split('/')[-1]) as t:\n",
        "                for chunk in r.iter_content(chunk_size):\n",
        "                    if chunk:\n",
        "                        file.write(chunk)\n",
        "                        t.update(len(chunk))\n",
        "        logging.info('File {} downloaded.'.format(path))\n",
        "\n",
        "        _check_hash(path)\n",
        "        return path\n",
        "\n",
        "    if path is None:\n",
        "        _, filename = os.path.split(url)\n",
        "    else:\n",
        "        root, filename = os.path.split(path)\n",
        "\n",
        "    if not os.path.exists(root):\n",
        "        try:\n",
        "            os.makedirs(root)\n",
        "        except OSError:\n",
        "            print(\"Can't create the download directory {}.\".format(root))\n",
        "            raise\n",
        "\n",
        "    if filename is not None:\n",
        "        path = os.path.join(root, filename)\n",
        "    # skip requests.get if path exists and not overwrite.\n",
        "    if os.path.exists(path):\n",
        "        logging.info('File %s already exists.' % path)\n",
        "        if not overwrite:\n",
        "            _check_hash(path)\n",
        "            return path\n",
        "\n",
        "    if 'drive.google.com' not in url:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, stream=True)\n",
        "        return _process_response(response, root, filename)\n",
        "    else:\n",
        "        # google drive links get filename from google drive\n",
        "        filename = None\n",
        "\n",
        "    logging.info('Downloading from Google Drive; may take a few minutes')\n",
        "    confirm_token = None\n",
        "    session = requests.Session()\n",
        "    response = session.get(url, stream=True)\n",
        "    for k, v in response.cookies.items():\n",
        "        if k.startswith(\"download_warning\"):\n",
        "            confirm_token = v\n",
        "\n",
        "    if confirm_token:\n",
        "        url = url + \"&confirm=\" + confirm_token\n",
        "        response = session.get(url, stream=True)\n",
        "\n",
        "    return _process_response(response, root, filename)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1jy6tHbnPUK"
      },
      "source": [
        "# Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoGC4UjSRiIb",
        "outputId": "e63f3228-148e-4951-fb49-2830de05fc20"
      },
      "source": [
        "import io\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "#from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
        "test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url,path=None))\n",
        "tokenizer = get_tokenizer('spacy')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer,\n",
        "                                      iter(io.open(train_filepath,\n",
        "                                                   encoding=\"utf8\"))))\n",
        "\n",
        "def data_process(raw_text_iter):\n",
        "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
        "                       dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
        "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
        "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 8.70MB/s]\n",
            "36718lines [00:13, 2796.99lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvJxucwiWNAx"
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFXrXy2LnT7w"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "-aJgh1B7SDyH",
        "outputId": "977ad014-d10e-4d03-9b53-760365d5fb45"
      },
      "source": [
        "#model\n",
        "ntokens = len(vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "\n",
        "### pos functions ###\n",
        "def safe_exp(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return torch.exp(x)\n",
        "\n",
        "def safe_softmax(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return F.softmax(x)\n",
        "\n",
        "def safe_sigmoid(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return F.sigmoid(x)\n",
        "\n",
        "def safe_avg_exp(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return torch.exp((x - x.mean()) / (x.std() + 1e-6))\n",
        "\n",
        "def safe_square(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return torch.square(x)\n",
        "\n",
        "def safe_abs(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return torch.abs(x)\n",
        "\n",
        "def safe_minus_exp(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return torch.exp(-x)\n",
        "\n",
        "def safe_square_abs(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return torch.square(x) / (torch.abs(x) + 1e-6)\n",
        "\n",
        "def safe_add_smallest(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return x + torch.min(x)\n",
        "def safe_tanh(x):\n",
        "  if x is None:\n",
        "    return None\n",
        "  else:\n",
        "    return 1 + torch.tanh(x)\n",
        "\n",
        "def safe_identity(x):\n",
        "  return x\n",
        "\n",
        "\n",
        "### activation functions ###\n",
        "\n",
        "def relu(xs):\n",
        "  return F.relu(xs)\n",
        "\n",
        "def tanh(xs):\n",
        "  return torch.tanh(xs)\n",
        "\n",
        "def tanh_plus_one(xs):\n",
        "  return torch.tanh(xs) + 1\n",
        "  \n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,purely_positive=True, posfn=safe_abs,act_fn = tanh_plus_one).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        if data.size(0) != bptt:\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if data.size(0) != bptt:\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "            output = eval_model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 3176 batches | lr 5.00 | ms/batch 20.49 | loss  8.59 | ppl  5367.36\n",
            "| epoch   1 |   400/ 3176 batches | lr 5.00 | ms/batch 20.22 | loss  7.45 | ppl  1724.38\n",
            "| epoch   1 |   600/ 3176 batches | lr 5.00 | ms/batch 20.21 | loss  7.28 | ppl  1456.24\n",
            "| epoch   1 |   800/ 3176 batches | lr 5.00 | ms/batch 20.22 | loss  7.12 | ppl  1238.21\n",
            "| epoch   1 |  1000/ 3176 batches | lr 5.00 | ms/batch 20.22 | loss  7.18 | ppl  1313.90\n",
            "| epoch   1 |  1200/ 3176 batches | lr 5.00 | ms/batch 20.21 | loss  7.16 | ppl  1281.34\n",
            "| epoch   1 |  1400/ 3176 batches | lr 5.00 | ms/batch 20.21 | loss  7.08 | ppl  1188.29\n",
            "| epoch   1 |  1600/ 3176 batches | lr 5.00 | ms/batch 20.21 | loss  7.05 | ppl  1148.82\n",
            "| epoch   1 |  1800/ 3176 batches | lr 5.00 | ms/batch 20.21 | loss  7.05 | ppl  1156.85\n",
            "| epoch   1 |  2000/ 3176 batches | lr 5.00 | ms/batch 20.24 | loss  7.09 | ppl  1196.21\n",
            "| epoch   1 |  2200/ 3176 batches | lr 5.00 | ms/batch 20.24 | loss  7.06 | ppl  1169.85\n",
            "| epoch   1 |  2400/ 3176 batches | lr 5.00 | ms/batch 20.23 | loss  7.02 | ppl  1113.21\n",
            "| epoch   1 |  2600/ 3176 batches | lr 5.00 | ms/batch 20.23 | loss  7.05 | ppl  1153.04\n",
            "| epoch   1 |  2800/ 3176 batches | lr 5.00 | ms/batch 20.22 | loss  7.03 | ppl  1126.63\n",
            "| epoch   1 |  3000/ 3176 batches | lr 5.00 | ms/batch 20.23 | loss  7.01 | ppl  1106.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 67.85s | valid loss  6.96 | valid ppl  1053.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 3176 batches | lr 4.51 | ms/batch 20.32 | loss  7.02 | ppl  1121.06\n",
            "| epoch   2 |   400/ 3176 batches | lr 4.51 | ms/batch 20.20 | loss  7.01 | ppl  1112.68\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-85ea67d7b820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-85ea67d7b820>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}